#!/usr/bin/env python3
"""
GGTH LSTM Backtest Data Generator
================================
Generates backtest_predictions.csv for MT5 Strategy Tester

This script:
1. Loads your trained LSTM models and scalers
2. Downloads historical EURUSD H1 data
3. Creates the same features as your EA
4. Generates predictions for each historical bar
5. Outputs CSV in exact format for MT5 backtesting

Usage: python generate_backtest_predictions.py
Output: backtest_predictions.csv (place in MT5/Common/Files/)
"""

import os
import pandas as pd
import numpy as np
import torch
import joblib
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# --- CONFIGURATION ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(SCRIPT_DIR, "models")
OUTPUT_FILE = "backtest_predictions.csv"

# Model parameters (must match your training script)
INPUT_FEATURES = 15
HIDDEN_SIZE, NUM_LAYERS, SEQ_LEN = 128, 3, 20
OUTPUT_STEPS = 5
NUM_CLASSES = 3

# Backtest parameters
BACKTEST_MONTHS = 6  # How many months of data to generate
MIN_CONFIDENCE = 0.3  # Minimum confidence for realistic backtesting

# --- ENHANCED MODEL ARCHITECTURE (must match daemon.py) ---
class AttentionLSTM(torch.nn.Module):
    """Enhanced LSTM with attention mechanism and uncertainty estimation"""
    
    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_regression_outputs, dropout=0.2):
        super(AttentionLSTM, self).__init__()
        
        # Input layer normalization
        self.input_norm = torch.nn.LayerNorm(input_size)
        
        # Enhanced LSTM
        self.lstm = torch.nn.LSTM(
            input_size, hidden_size, num_layers, 
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # Layer normalization for LSTM output
        self.lstm_norm = torch.nn.LayerNorm(hidden_size)
        
        # Multi-head self-attention
        self.attention = torch.nn.MultiheadAttention(
            hidden_size, num_heads=8, dropout=dropout, batch_first=True
        )
        
        # Attention normalization
        self.attention_norm = torch.nn.LayerNorm(hidden_size)
        
        # Feature fusion layer
        self.fusion = torch.nn.Sequential(
            torch.nn.Linear(hidden_size * 2, hidden_size),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout)
        )
        
        # Separate specialized heads
        self.regression_head = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, hidden_size // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout * 0.5),
            torch.nn.Linear(hidden_size // 2, hidden_size // 4),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size // 4, num_regression_outputs)
        )
        
        self.classification_head = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, hidden_size // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout * 0.5),
            torch.nn.Linear(hidden_size // 2, hidden_size // 4),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size // 4, num_classes)
        )
        
        # Uncertainty estimation head
        self.uncertainty_head = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, hidden_size // 2),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size // 2, num_regression_outputs)
        )
        
        # Model confidence head
        self.confidence_head = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, hidden_size // 4),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size // 4, 1),
            torch.nn.Sigmoid()
        )
        
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # Input normalization
        x = self.input_norm(x)
        
        # LSTM processing
        lstm_out, (h_n, c_n) = self.lstm(x)
        lstm_out = self.lstm_norm(lstm_out)
        
        # Self-attention mechanism
        attn_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)
        attn_out = self.attention_norm(attn_out)
        
        # Feature fusion: combine LSTM and attention outputs
        combined = torch.cat([lstm_out, attn_out], dim=-1)
        fused = self.fusion(combined)
        
        # Aggregate temporal information
        last_hidden = fused[:, -1, :]  # Last timestep
        avg_hidden = torch.mean(fused, dim=1)  # Average across time
        max_hidden = torch.max(fused, dim=1)[0]  # Max pooling
        
        # Multi-scale feature aggregation
        final_hidden = (last_hidden + avg_hidden + max_hidden) / 3
        
        # Generate outputs
        regression_output = self.regression_head(final_hidden)
        classification_logits = self.classification_head(final_hidden)
        uncertainty = torch.exp(self.uncertainty_head(final_hidden))  # Positive uncertainty
        model_confidence = self.confidence_head(final_hidden)
        
        return regression_output, classification_logits, uncertainty, model_confidence, attention_weights

# --- FALLBACK MODEL (in case enhanced model fails to load) ---
class CombinedLSTM(torch.nn.Module):
    """Fallback model matching original architecture"""
    
    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_regression_outputs):
        super(CombinedLSTM, self).__init__()
        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, 
                           dropout=0.2 if num_layers > 1 else 0)
        self.fc_regression = torch.nn.Linear(hidden_size, num_regression_outputs)
        self.fc_classification = torch.nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        out, _ = self.lstm(x)
        last_hidden_state = out[:, -1, :]
        regression_output = self.fc_regression(last_hidden_state)
        classification_logits = self.fc_classification(last_hidden_state)
        return regression_output, classification_logits

class BacktestGenerator:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üöÄ Using device: {self.device}")
        
        self.model = None
        self.ensemble_models = []
        self.scaler_feature = None
        self.scaler_regressor_target = None
        self.model_type = None
        
        self._load_models()
        
    def _load_models(self):
        """Load trained models and scalers"""
        print("üîÑ Loading models and scalers...")
        
        try:
            # Load scalers
            scaler_feature_path = os.path.join(MODEL_DIR, "scaler.pkl")
            scaler_target_path = os.path.join(MODEL_DIR, "scaler_regression.pkl")
            model_path = os.path.join(MODEL_DIR, "lstm_model_regression.pth")
            
            if not os.path.exists(scaler_feature_path):
                raise FileNotFoundError(f"Feature scaler not found: {scaler_feature_path}")
            if not os.path.exists(scaler_target_path):
                raise FileNotFoundError(f"Target scaler not found: {scaler_target_path}")
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"Model not found: {model_path}")
                
            self.scaler_feature = joblib.load(scaler_feature_path)
            self.scaler_regressor_target = joblib.load(scaler_target_path)
            print(f"‚úÖ Scalers loaded successfully")
            
            # Try to load enhanced model first
            try:
                self.model = AttentionLSTM(INPUT_FEATURES, HIDDEN_SIZE, NUM_LAYERS, 
                                         NUM_CLASSES, OUTPUT_STEPS)
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state'])
                self.model.to(self.device).eval()
                self.model_type = "enhanced"
                print(f"‚úÖ Enhanced AttentionLSTM model loaded")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Enhanced model failed, trying original: {e}")
                try:
                    self.model = CombinedLSTM(INPUT_FEATURES, HIDDEN_SIZE, 2, 
                                            NUM_CLASSES, OUTPUT_STEPS)
                    checkpoint = torch.load(model_path, map_location=self.device)
                    self.model.load_state_dict(checkpoint['model_state'])
                    self.model.to(self.device).eval()
                    self.model_type = "original"
                    print(f"‚úÖ Original CombinedLSTM model loaded")
                except Exception as e2:
                    raise RuntimeError(f"Both model architectures failed: {e2}")
            
            # Load ensemble models
            self._load_ensemble_models()
            
        except Exception as e:
            print(f"üí• FATAL: Could not load models/scalers: {e}")
            raise
            
    def _load_ensemble_models(self):
        """Load ensemble models if available"""
        ensemble_count = 0
        
        for i in range(1, 6):  # Try to load up to 5 ensemble models
            ensemble_path = os.path.join(MODEL_DIR, f"lstm_ensemble_{i}.pth")
            if os.path.exists(ensemble_path):
                try:
                    if self.model_type == "enhanced":
                        ensemble_model = AttentionLSTM(INPUT_FEATURES, HIDDEN_SIZE, NUM_LAYERS, 
                                                     NUM_CLASSES, OUTPUT_STEPS)
                    else:
                        ensemble_model = CombinedLSTM(INPUT_FEATURES, HIDDEN_SIZE, 2, 
                                                    NUM_CLASSES, OUTPUT_STEPS)
                    
                    checkpoint = torch.load(ensemble_path, map_location=self.device)
                    ensemble_model.load_state_dict(checkpoint['model_state'])
                    ensemble_model.to(self.device).eval()
                    self.ensemble_models.append(ensemble_model)
                    ensemble_count += 1
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to load ensemble model {i}: {e}")
                    continue
        
        if ensemble_count > 0:
            print(f"‚úÖ Loaded {ensemble_count} ensemble models")
        else:
            print("‚ÑπÔ∏è  No ensemble models found (using single model)")
            
    def download_data(self):
        """Download historical EURUSD data"""
        print("üìä Downloading historical EURUSD data...")
        
        # Calculate date range
        end_date = datetime.now()
        start_date = end_date - timedelta(days=BACKTEST_MONTHS * 30 + 60)  # Extra margin for feature calculation
        
        try:
            # Download EURUSD data from Yahoo Finance
            ticker = "EURUSD=X"
            data = yf.download(ticker, start=start_date, end=end_date, interval="1h", progress=False)
            
            if data.empty:
                raise ValueError("No data downloaded from Yahoo Finance")
                
            # Clean and prepare data
            data = data.dropna()
            data.index = pd.to_datetime(data.index)
            
            # Ensure we have OHLC columns
            required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            for col in required_columns:
                if col not in data.columns:
                    if col == 'Volume':
                        data[col] = 1000  # Dummy volume for forex
                    else:
                        raise ValueError(f"Missing required column: {col}")
            
            print(f"‚úÖ Downloaded {len(data)} hourly bars from {data.index[0]} to {data.index[-1]}")
            return data
            
        except Exception as e:
            print(f"üí• Failed to download data: {e}")
            print("üí° Trying alternative data source...")
            
            # Fallback: Generate synthetic data for testing
            return self._generate_synthetic_data(start_date, end_date)
    
    def _generate_synthetic_data(self, start_date, end_date):
        """Generate synthetic EURUSD data for testing"""
        print("üîß Generating synthetic EURUSD data...")
        
        # Create hourly datetime index
        dates = pd.date_range(start=start_date, end=end_date, freq='H')
        
        # Generate realistic EURUSD price movement
        base_price = 1.14
        num_bars = len(dates)
        
        # Random walk with mean reversion
        returns = np.random.normal(0, 0.0005, num_bars)  # 0.05% hourly volatility
        prices = [base_price]
        
        for i in range(1, num_bars):
            # Mean reversion to 1.14
            mean_reversion = (base_price - prices[-1]) * 0.001
            new_price = prices[-1] + returns[i] + mean_reversion
            prices.append(max(1.10, min(1.18, new_price)))  # Keep in realistic range
        
        # Create OHLC data
        data = pd.DataFrame(index=dates)
        data['Close'] = prices
        data['Open'] = data['Close'].shift(1).fillna(data['Close'].iloc[0])
        data['High'] = data[['Open', 'Close']].max(axis=1) + np.random.uniform(0, 0.0003, num_bars)
        data['Low'] = data[['Open', 'Close']].min(axis=1) - np.random.uniform(0, 0.0003, num_bars)
        data['Volume'] = np.random.randint(1000, 5000, num_bars)
        
        print(f"‚úÖ Generated {len(data)} synthetic bars")
        return data
    
    def create_features(self, data):
        """Create features matching EA's feature engineering"""
        print("üîß Creating features...")
        
        df = data.copy()
        
        # Technical indicators
        # RSI
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema12 = df['Close'].ewm(span=12).mean()
        ema26 = df['Close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        
        # Stochastic
        lowest_low = df['Low'].rolling(14).min()
        highest_high = df['High'].rolling(14).max()
        df['stoch_k'] = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)
        
        # CCI
        tp = (df['High'] + df['Low'] + df['Close']) / 3
        ma_tp = tp.rolling(20).mean()
        mad = tp.rolling(20).apply(lambda x: np.mean(np.abs(x - x.mean())))
        df['cci'] = (tp - ma_tp) / (0.015 * mad)
        
        # ATR
        high_low = df['High'] - df['Low']
        high_close = abs(df['High'] - df['Close'].shift(1))
        low_close = abs(df['Low'] - df['Close'].shift(1))
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df['atr'] = true_range.rolling(14).mean()
        
        # Bollinger Bands
        bb_ma = df['Close'].rolling(20).mean()
        bb_std = df['Close'].rolling(20).std()
        df['bb_upper'] = bb_ma + (bb_std * 2)
        df['bb_lower'] = bb_ma - (bb_std * 2)
        
        # Time features
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        
        # Price returns
        df['price_return'] = df['Close'].pct_change()
        
        # Currency basket features (simplified - using EUR and USD proxies)
        df['usd_strength'] = df['Close'].pct_change().rolling(5).mean()  # Simplified USD strength
        df['eur_strength'] = -df['usd_strength']  # Inverse for EUR
        df['jpy_strength'] = df['Close'].pct_change().rolling(3).mean() * 0.5  # Simplified JPY
        
        # Volume features
        df['volume_change'] = df['Volume'] - df['Volume'].shift(5)
        
        # Candlestick patterns
        body = abs(df['Close'] - df['Open'])
        range_size = df['High'] - df['Low']
        df['candle_type'] = 0
        
        # Doji-like patterns
        doji_mask = (range_size > 0) & (body / range_size < 0.1)
        df.loc[doji_mask, 'candle_type'] = 1
        
        # Engulfing patterns (simplified)
        bullish_engulf = (df['Close'] > df['Open']) & (df['Open'] < df['Open'].shift(1)) & (df['Close'] > df['Close'].shift(1))
        bearish_engulf = (df['Close'] < df['Open']) & (df['Open'] > df['Open'].shift(1)) & (df['Close'] < df['Close'].shift(1))
        
        df.loc[bullish_engulf, 'candle_type'] = 2
        df.loc[bearish_engulf, 'candle_type'] = -2
        
        # Add gap component
        gap = (df['Open'] - df['Close'].shift(1)) / (df['atr'] + 1e-10)
        df['candle_type'] += gap
        
        print(f"‚úÖ Created features for {len(df)} bars")
        return df
    
    def create_sequences(self, df):
        """Create sequences for model prediction"""
        print("üîó Creating sequences...")
        
        # Define feature columns in same order as EA
        feature_columns = [
            'price_return', 'Volume', 'atr', 'macd', 'rsi', 'stoch_k', 'cci',
            'hour', 'day_of_week', 'usd_strength', 'eur_strength', 'jpy_strength',
            'bb_upper', 'volume_change', 'candle_type'
        ]
        
        # Ensure we have all required features
        for col in feature_columns:
            if col not in df.columns:
                print(f"‚ö†Ô∏è  Missing feature: {col}, filling with zeros")
                df[col] = 0
        
        # Handle missing values
        df = df.fillna(method='ffill').fillna(0)
        
        # Create sequences
        sequences = []
        timestamps = []
        current_prices = []
        
        for i in range(SEQ_LEN, len(df)):
            # Get sequence of features
            sequence_data = df[feature_columns].iloc[i-SEQ_LEN:i].values
            
            # Ensure we have exactly the right number of features
            if sequence_data.shape[1] != INPUT_FEATURES:
                print(f"‚ö†Ô∏è  Feature count mismatch: expected {INPUT_FEATURES}, got {sequence_data.shape[1]}")
                continue
            
            sequences.append(sequence_data)
            timestamps.append(df.index[i])
            current_prices.append(df['Close'].iloc[i])
        
        sequences = np.array(sequences)
        print(f"‚úÖ Created {len(sequences)} sequences of shape {sequences.shape}")
        
        return sequences, timestamps, current_prices
    
    def generate_predictions(self, sequences, current_prices):
        """Generate predictions for all sequences"""
        print("üéØ Generating predictions...")
        
        all_predictions = []
        
        # Process in batches to avoid memory issues
        batch_size = 100
        num_batches = (len(sequences) + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(sequences))
            
            batch_sequences = sequences[start_idx:end_idx]
            batch_current_prices = current_prices[start_idx:end_idx]
            
            # Scale features
            batch_scaled = self.scaler_feature.transform(
                batch_sequences.reshape(-1, INPUT_FEATURES)
            ).reshape(batch_sequences.shape)
            
            # Convert to tensor
            batch_tensor = torch.tensor(batch_scaled, dtype=torch.float32).to(self.device)
            
            # Generate predictions
            with torch.no_grad():
                if len(self.ensemble_models) > 0:
                    # Use ensemble prediction
                    batch_predictions = self._get_ensemble_predictions(batch_tensor, batch_current_prices)
                else:
                    # Use single model
                    batch_predictions = self._get_single_predictions(batch_tensor, batch_current_prices)
            
            all_predictions.extend(batch_predictions)
            
            if (batch_idx + 1) % 10 == 0:
                print(f"   Processed {batch_idx + 1}/{num_batches} batches...")
        
        print(f"‚úÖ Generated {len(all_predictions)} predictions")
        return all_predictions
    
    def _get_ensemble_predictions(self, batch_tensor, batch_current_prices):
        """Generate ensemble predictions"""
        models_to_use = [self.model] + self.ensemble_models
        batch_predictions = []
        
        for i in range(len(batch_tensor)):
            all_model_predictions = []
            all_model_classifications = []
            all_model_confidences = []
            
            single_tensor = batch_tensor[i:i+1]
            current_price = batch_current_prices[i]
            
            # Get predictions from all models
            for model in models_to_use:
                try:
                    if self.model_type == "enhanced":
                        reg_out, class_logits, uncertainty, confidence, _ = model(single_tensor)
                        model_confidence = float(confidence.cpu().numpy().item())
                    else:
                        reg_out, class_logits = model(single_tensor)
                        model_confidence = 0.7  # Default confidence for original model
                    
                    predictions = reg_out.cpu().numpy()[0]
                    classification_probs = torch.softmax(class_logits, dim=1)[0].cpu().numpy()
                    
                    all_model_predictions.append(predictions)
                    all_model_classifications.append(classification_probs)
                    all_model_confidences.append(model_confidence)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  Model failed: {e}")
                    continue
            
            if not all_model_predictions:
                print("‚ö†Ô∏è  All models failed for this sample")
                continue
            
            # Ensemble the predictions
            ensemble_predictions = np.mean(all_model_predictions, axis=0)
            ensemble_classification = np.mean(all_model_classifications, axis=0)
            ensemble_confidence = np.mean(all_model_confidences)
            
            # Denormalize predictions
            try:
                unscaled_predictions = self.scaler_regressor_target.inverse_transform(
                    ensemble_predictions.reshape(1, -1)
                )[0]
                
                # Check if denormalization is realistic
                if not all(0.5 < price < 2.5 for price in unscaled_predictions):
                    # Use ATR-based scaling fallback
                    atr_estimate = current_price * 0.002  # Rough ATR estimate
                    unscaled_predictions = []
                    for pred in ensemble_predictions:
                        atr_scaled_change = pred * atr_estimate * 2.0
                        future_price = current_price + atr_scaled_change
                        unscaled_predictions.append(float(future_price))
                        
            except Exception as e:
                print(f"‚ö†Ô∏è  Denormalization failed: {e}")
                # Fallback to small movements around current price
                unscaled_predictions = [current_price + (pred * 0.01) for pred in ensemble_predictions]
            
            # Extract classification probabilities
            sell_prob, hold_prob, buy_prob = ensemble_classification
            
            batch_predictions.append({
                'predicted_prices': unscaled_predictions,
                'buy_prob': float(buy_prob),
                'sell_prob': float(sell_prob),
                'hold_prob': float(hold_prob),
                'confidence': max(MIN_CONFIDENCE, ensemble_confidence)  # Ensure minimum confidence
            })
        
        return batch_predictions
    
    def _get_single_predictions(self, batch_tensor, batch_current_prices):
        """Generate single model predictions"""
        batch_predictions = []
        
        for i in range(len(batch_tensor)):
            single_tensor = batch_tensor[i:i+1]
            current_price = batch_current_prices[i]
            
            try:
                if self.model_type == "enhanced":
                    reg_out, class_logits, uncertainty, confidence, _ = self.model(single_tensor)
                    model_confidence = float(confidence.cpu().numpy().item())
                else:
                    reg_out, class_logits = self.model(single_tensor)
                    model_confidence = 0.7
                
                predictions = reg_out.cpu().numpy()[0]
                classification_probs = torch.softmax(class_logits, dim=1)[0].cpu().numpy()
                
                # Denormalize predictions (same logic as ensemble)
                try:
                    unscaled_predictions = self.scaler_regressor_target.inverse_transform(
                        predictions.reshape(1, -1)
                    )[0]
                    
                    if not all(0.5 < price < 2.5 for price in unscaled_predictions):
                        atr_estimate = current_price * 0.002
                        unscaled_predictions = []
                        for pred in predictions:
                            atr_scaled_change = pred * atr_estimate * 2.0
                            future_price = current_price + atr_scaled_change
                            unscaled_predictions.append(float(future_price))
                            
                except Exception as e:
                    unscaled_predictions = [current_price + (pred * 0.01) for pred in predictions]
                
                sell_prob, hold_prob, buy_prob = classification_probs
                
                batch_predictions.append({
                    'predicted_prices': unscaled_predictions,
                    'buy_prob': float(buy_prob),
                    'sell_prob': float(sell_prob),
                    'hold_prob': float(hold_prob),
                    'confidence': max(MIN_CONFIDENCE, model_confidence)
                })
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Single model prediction failed: {e}")
                continue
        
        return batch_predictions
    
    def save_csv(self, timestamps, predictions):
        """Save predictions in MT5 backtest format"""
        print("üíæ Saving CSV file...")
        
        # Create DataFrame
        rows = []
        for i, (timestamp, pred) in enumerate(zip(timestamps, predictions)):
            # Format timestamp for MT5 (YYYY.MM.DD HH:MM:SS)
            mt5_timestamp = timestamp.strftime("%Y.%m.%d %H:%M:%S")
            
            # Create row: timestamp;buy_prob;sell_prob;hold_prob;confidence_score;price1;price2;price3;price4;price5
            row = [
                mt5_timestamp,
                pred['buy_prob'],
                pred['sell_prob'],
                pred['hold_prob'],
                pred['confidence']
            ]
            
            # Add predicted prices
            for price in pred['predicted_prices']:
                row.append(price)
            
            rows.append(row)
        
        # Create DataFrame
        columns = ['timestamp', 'buy_prob', 'sell_prob', 'hold_prob', 'confidence_score'] + \
                 [f'price_{i+1}' for i in range(OUTPUT_STEPS)]
        
        df = pd.DataFrame(rows, columns=columns)
        
        # Save with semicolon delimiter (as expected by EA)
        output_path = os.path.join(SCRIPT_DIR, OUTPUT_FILE)
        df.to_csv(output_path, sep=';', index=False, float_format='%.8f')
        
        print(f"‚úÖ Saved {len(df)} predictions to: {output_path}")
        print(f"üìã File format: {df.shape[0]} rows √ó {df.shape[1]} columns")
        print(f"üìä Confidence range: {df['confidence_score'].min():.3f} - {df['confidence_score'].max():.3f}")
        print(f"üìà Price range: {df['price_1'].min():.5f} - {df['price_1'].max():.5f}")
        
        return output_path
    
    def generate_backtest_data(self):
        """Main function to generate complete backtest data"""
        print(f"\nüöÄ Generating backtest data for {BACKTEST_MONTHS} months...")
        print("=" * 60)
        
        try:
            # Download historical data
            data = self.download_data()
            
            # Create features
            df_with_features = self.create_features(data)
            
            # Create sequences
            sequences, timestamps, current_prices = self.create_sequences(df_with_features)
            
            if len(sequences) == 0:
                raise ValueError("No valid sequences created")
            
            # Generate predictions
            predictions = self.generate_predictions(sequences, current_prices)
            
            if len(predictions) == 0:
                raise ValueError("No predictions generated")
            
            # Save CSV
            output_path = self.save_csv(timestamps, predictions)
            
            print("\n" + "=" * 60)
            print("üéâ BACKTEST DATA GENERATION COMPLETE!")
            print("=" * 60)
            print(f"üìÅ Output file: {output_path}")
            print(f"üìä Total predictions: {len(predictions)}")
            print(f"üìÖ Date range: {timestamps[0].strftime('%Y-%m-%d')} to {timestamps[-1].strftime('%Y-%m-%d')}")
            print("\nüìã Next steps:")
            print(f"1. Copy {OUTPUT_FILE} to your MT5/Common/Files/ folder")
            print("2. Open MT5 Strategy Tester")
            print("3. Select your GGTH EA")
            print("4. Set date range within the generated data period")
            print("5. Run backtest!")
            print("\nüéØ The EA will now use real model predictions for backtesting!")
            
            return output_path
            
        except Exception as e:
            print(f"\nüí• FATAL ERROR: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """Main execution function"""
    print("üéØ GGTH LSTM Backtest Data Generator")
    print("=" * 50)
    
    try:
        generator = BacktestGenerator()
        result = generator.generate_backtest_data()
        
        if result:
            print(f"\n‚úÖ SUCCESS! Backtest data ready at: {result}")
        else:
            print(f"\n‚ùå FAILED! Check error messages above.")
            
    except KeyboardInterrupt:
        print(f"\nüõë Generation interrupted by user")
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()