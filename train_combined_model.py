import os
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import joblib
import sys
import traceback
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# --- CONFIGURATION ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_SAVE_PATH = os.path.join(SCRIPT_DIR, "models")
DATA_DIR = SCRIPT_DIR

# Model parameters
INPUT_FEATURES = 15
HIDDEN_SIZE = 128
NUM_LAYERS = 3
SEQ_LEN = 20
OUTPUT_STEPS = 5
NUM_CLASSES = 3
EPOCHS = 50
BATCH_SIZE = 64
LEARNING_RATE = 0.001
LOOKAHEAD_BARS = 5
PROFIT_THRESHOLD_ATR = 0.75

# Training configuration
TRAIN_ENHANCED = True  # Set to False for original model
CREATE_ENSEMBLE = True  # Create ensemble models
VALIDATION_SPLIT = 0.8  # 80% train, 20% validation

print(f"üöÄ Enhanced LSTM Training Configuration:")
print(f"   üìä Model: {'AttentionLSTM' if TRAIN_ENHANCED else 'CombinedLSTM'}")
print(f"   üé≠ Ensemble: {'Yes' if CREATE_ENSEMBLE else 'No'}")
print(f"   üìà Features: {INPUT_FEATURES}")
print(f"   üîß Hidden Size: {HIDDEN_SIZE}")
print(f"   üìö Layers: {NUM_LAYERS}")
print(f"   ‚è∞ Sequence Length: {SEQ_LEN}")
print(f"   üéØ Output Steps: {OUTPUT_STEPS}")
print(f"   üìö Epochs: {EPOCHS}")
print(f"   üì¶ Batch Size: {BATCH_SIZE}")

# --- ENHANCED MODEL ARCHITECTURE ---
class AttentionLSTM(nn.Module):
    """Enhanced LSTM with attention mechanism and uncertainty estimation"""
    
    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_regression_outputs, dropout=0.2):
        super(AttentionLSTM, self).__init__()
        
        # Input layer normalization
        self.input_norm = nn.LayerNorm(input_size)
        
        # Enhanced LSTM
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers, 
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # Layer normalization for LSTM output
        self.lstm_norm = nn.LayerNorm(hidden_size)
        
        # Multi-head self-attention
        self.attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, dropout=dropout, batch_first=True
        )
        
        # Attention normalization
        self.attention_norm = nn.LayerNorm(hidden_size)
        
        # Feature fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Separate specialized heads
        self.regression_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, num_regression_outputs)
        )
        
        self.classification_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, num_classes)
        )
        
        # Uncertainty estimation head
        self.uncertainty_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, num_regression_outputs)
        )
        
        # Model confidence head
        self.confidence_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # Input normalization
        x = self.input_norm(x)
        
        # LSTM processing
        lstm_out, (h_n, c_n) = self.lstm(x)
        lstm_out = self.lstm_norm(lstm_out)
        
        # Self-attention mechanism
        attn_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)
        attn_out = self.attention_norm(attn_out)
        
        # Feature fusion: combine LSTM and attention outputs
        combined = torch.cat([lstm_out, attn_out], dim=-1)
        fused = self.fusion(combined)
        
        # Aggregate temporal information
        last_hidden = fused[:, -1, :]  # Last timestep
        avg_hidden = torch.mean(fused, dim=1)  # Average across time
        max_hidden = torch.max(fused, dim=1)[0]  # Max pooling
        
        # Multi-scale feature aggregation
        final_hidden = (last_hidden + avg_hidden + max_hidden) / 3
        
        # Generate outputs
        regression_output = self.regression_head(final_hidden)
        classification_logits = self.classification_head(final_hidden)
        uncertainty = torch.exp(self.uncertainty_head(final_hidden))  # Positive uncertainty
        model_confidence = self.confidence_head(final_hidden)
        
        return regression_output, classification_logits, uncertainty, model_confidence, attention_weights

# --- ORIGINAL MODEL FOR COMPARISON ---
class CombinedLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_regression_outputs):
        super(CombinedLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2 if num_layers > 1 else 0)
        self.fc_regression = nn.Linear(hidden_size, num_regression_outputs)
        self.fc_classification = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        out, _ = self.lstm(x)
        last_hidden_state = out[:, -1, :]
        regression_output = self.fc_regression(last_hidden_state)
        classification_logits = self.fc_classification(last_hidden_state)
        return regression_output, classification_logits

# --- ENHANCED LOSS FUNCTIONS ---
class UncertaintyLoss(nn.Module):
    """Loss function that incorporates uncertainty estimation"""
    
    def __init__(self):
        super(UncertaintyLoss, self).__init__()
        
    def forward(self, predictions, targets, uncertainties):
        # Uncertainty-weighted MSE loss
        mse_loss = torch.mean((predictions - targets) ** 2 / (2 * uncertainties + 1e-6) + 0.5 * torch.log(uncertainties + 1e-6))
        return mse_loss

# --- DATA LOADING AND PROCESSING ---
def load_and_process_data():
    """Load and process the corrected CSV data"""
    
    print(f"\nüìä LOADING AND PROCESSING DATA")
    print("-" * 50)
    
    # Look for the main data file
    data_files = ['EURUSD60.csv', 'eurusd60.csv', 'EURUSD.csv']
    data_file = None
    
    for file in data_files:
        if os.path.exists(os.path.join(DATA_DIR, file)):
            data_file = file
            break
    
    if not data_file:
        print("‚ùå No EURUSD data file found!")
        print(f"   Searched for: {data_files}")
        print(f"   In directory: {DATA_DIR}")
        return None, None
    
    print(f"üìÅ Loading data from: {data_file}")
    
    try:
        # Load the CSV file
        df = pd.read_csv(os.path.join(DATA_DIR, data_file))
        
        print(f"üìä Loaded {len(df):,} records")
        print(f"üîß Columns: {list(df.columns)}")
        
        # Check if this is the corrected format
        if 'Date' in df.columns:
            print("‚úÖ Using timezone-corrected CSV format")
            df['Date'] = pd.to_datetime(df['Date'])
        else:
            print("‚ùå Expected 'Date' column not found")
            print("   Please run fix_csv_timezone.py first to correct the data format")
            return None, None
        
        # Show data range
        print(f"üìÖ Date range: {df['Date'].min()} to {df['Date'].max()}")
        
        # Verify required columns
        required_cols = ['Open', 'High', 'Low', 'Close']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"‚ùå Missing required columns: {missing_cols}")
            return None, None
        
        print(f"üí∞ Price range: {df['Low'].min():.5f} - {df['High'].max():.5f}")
        
        return df, data_file
        
    except Exception as e:
        print(f"üí• Error loading data: {e}")
        traceback.print_exc()
        return None, None

def create_features(df):
    """Create technical indicators and features"""
    
    print(f"\nüîß CREATING FEATURES")
    print("-" * 50)
    
    try:
        # Make a copy to avoid modifying original
        data = df.copy()
        
        # Sort by date to ensure proper order
        data = data.sort_values('Date').reset_index(drop=True)
        
        print(f"üìà Creating technical indicators...")
        
        # Price returns (5 features)
        for i in range(1, 6):
            data[f'return_{i}'] = data['Close'].pct_change(i)
        
        # Moving averages (3 features)
        data['ma_5'] = data['Close'].rolling(5).mean()
        data['ma_10'] = data['Close'].rolling(10).mean()
        data['ma_20'] = data['Close'].rolling(20).mean()
        
        # MA ratios
        data['close_ma5_ratio'] = data['Close'] / data['ma_5'] - 1
        data['close_ma10_ratio'] = data['Close'] / data['ma_10'] - 1
        data['close_ma20_ratio'] = data['Close'] / data['ma_20'] - 1
        
        # Volatility indicators (2 features)
        data['volatility_10'] = data['return_1'].rolling(10).std()
        data['atr'] = (data['High'] - data['Low']).rolling(14).mean()
        data['atr_ratio'] = data['atr'] / data['Close']
        
        # RSI (1 feature)
        delta = data['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        data['rsi'] = 100 - (100 / (1 + rs))
        data['rsi_normalized'] = (data['rsi'] - 50) / 50
        
        # MACD (1 feature)
        ema_12 = data['Close'].ewm(span=12).mean()
        ema_26 = data['Close'].ewm(span=26).mean()
        data['macd'] = ema_12 - ema_26
        data['macd_ratio'] = data['macd'] / data['Close']
        
        # Time features (2 features)
        data['hour'] = data['Date'].dt.hour
        data['day_of_week'] = data['Date'].dt.dayofweek
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        
        # Select final feature set (15 features)
        feature_columns = [
            'return_1', 'return_2', 'return_3', 'return_4', 'return_5',
            'close_ma5_ratio', 'close_ma10_ratio', 'close_ma20_ratio',
            'volatility_10', 'atr_ratio',
            'rsi_normalized', 'macd_ratio',
            'hour_sin', 'day_sin'
        ]
        
        # Add one more feature to make 15
        data['price_position'] = (data['Close'] - data['Low']) / (data['High'] - data['Low'])
        feature_columns.append('price_position')
        
        print(f"‚úÖ Created {len(feature_columns)} features:")
        for i, feat in enumerate(feature_columns):
            print(f"   {i+1:2d}. {feat}")
        
        # Remove rows with NaN values
        print(f"üìä Data before cleaning: {len(data):,} records")
        data = data.dropna()
        print(f"üìä Data after cleaning: {len(data):,} records")
        
        if len(data) < SEQ_LEN + OUTPUT_STEPS:
            print(f"‚ùå Insufficient data after cleaning: {len(data)} records")
            return None, None
        
        return data, feature_columns
        
    except Exception as e:
        print(f"üí• Error creating features: {e}")
        traceback.print_exc()
        return None, None

def create_targets(df):
    """Create regression and classification targets"""
    
    print(f"\nüéØ CREATING TARGETS")
    print("-" * 50)
    
    try:
        data = df.copy()
        
        # Regression targets (future prices)
        print(f"üìà Creating regression targets for {OUTPUT_STEPS} future steps...")
        for i in range(1, OUTPUT_STEPS + 1):
            data[f'target_regr_{i}'] = data['Close'].shift(-i)
        
        # Classification targets (direction prediction)
        print(f"üìä Creating classification targets...")
        future_price = data['Close'].shift(-LOOKAHEAD_BARS)
        atr_threshold = data['atr'] * PROFIT_THRESHOLD_ATR
        
        conditions = [
            future_price > data['Close'] + atr_threshold,  # Strong up move
            future_price < data['Close'] - atr_threshold   # Strong down move
        ]
        choices = [2, 0]  # 2=Buy, 0=Sell, 1=Hold (default)
        
        data['target_class'] = np.select(conditions, choices, default=1)
        
        # Show class distribution
        class_counts = np.bincount(data['target_class'].dropna().astype(int))
        total = len(data['target_class'].dropna())
        
        print(f"üìä Class distribution:")
        print(f"   Sell (0): {class_counts[0]:,} ({class_counts[0]/total*100:.1f}%)")
        print(f"   Hold (1): {class_counts[1]:,} ({class_counts[1]/total*100:.1f}%)")
        print(f"   Buy  (2): {class_counts[2]:,} ({class_counts[2]/total*100:.1f}%)")
        
        return data
        
    except Exception as e:
        print(f"üí• Error creating targets: {e}")
        traceback.print_exc()
        return None

def prepare_sequences(data, feature_columns):
    """Prepare sequences for LSTM training"""
    
    print(f"\nüîó PREPARING SEQUENCES")
    print("-" * 50)
    
    try:
        # Remove rows with NaN targets
        data_clean = data.dropna()
        print(f"üìä Clean data: {len(data_clean):,} records")
        
        # Extract features and targets
        X = data_clean[feature_columns].values
        y_regr_columns = [f'target_regr_{i}' for i in range(1, OUTPUT_STEPS + 1)]
        y_regr = data_clean[y_regr_columns].values
        y_class = data_clean['target_class'].values
        
        print(f"üìà Feature matrix shape: {X.shape}")
        print(f"üéØ Regression targets shape: {y_regr.shape}")
        print(f"üìä Classification targets shape: {y_class.shape}")
        
        # Scale features
        print(f"‚öñÔ∏è  Scaling features...")
        feature_scaler = StandardScaler()
        X_scaled = feature_scaler.fit_transform(X)
        
        # Scale regression targets
        print(f"‚öñÔ∏è  Scaling regression targets...")
        target_scaler = StandardScaler()
        y_regr_scaled = target_scaler.fit_transform(y_regr)
        
        # Create sequences
        print(f"üîó Building sequences of length {SEQ_LEN}...")
        X_seq, y_regr_seq, y_class_seq = [], [], []
        
        for i in range(len(X_scaled) - SEQ_LEN):
            X_seq.append(X_scaled[i:i + SEQ_LEN])
            y_regr_seq.append(y_regr_scaled[i + SEQ_LEN - 1])
            y_class_seq.append(y_class[i + SEQ_LEN - 1])
            
            # Progress indicator
            if (i + 1) % 10000 == 0:
                print(f"   üìà Created {i + 1:,} sequences...")
        
        # Convert to tensors
        X_tensor = torch.tensor(np.array(X_seq), dtype=torch.float32)
        y_regr_tensor = torch.tensor(np.array(y_regr_seq), dtype=torch.float32)
        y_class_tensor = torch.tensor(np.array(y_class_seq), dtype=torch.long)
        
        print(f"‚úÖ Created {len(X_tensor):,} sequences")
        print(f"üìä Sequence shapes:")
        print(f"   Features: {X_tensor.shape}")
        print(f"   Regression: {y_regr_tensor.shape}")
        print(f"   Classification: {y_class_tensor.shape}")
        
        return X_tensor, y_regr_tensor, y_class_tensor, feature_scaler, target_scaler
        
    except Exception as e:
        print(f"üí• Error preparing sequences: {e}")
        traceback.print_exc()
        return None, None, None, None, None

def create_data_loaders(X_tensor, y_regr_tensor, y_class_tensor):
    """Create train and validation data loaders"""
    
    print(f"\nüì¶ CREATING DATA LOADERS")
    print("-" * 50)
    
    try:
        # Split data chronologically (time series split)
        train_size = int(VALIDATION_SPLIT * len(X_tensor))
        
        # Training data
        X_train = X_tensor[:train_size]
        y_regr_train = y_regr_tensor[:train_size]
        y_class_train = y_class_tensor[:train_size]
        
        # Validation data
        X_val = X_tensor[train_size:]
        y_regr_val = y_regr_tensor[train_size:]
        y_class_val = y_class_tensor[train_size:]
        
        print(f"üìä Data split:")
        print(f"   Training: {len(X_train):,} sequences ({len(X_train)/len(X_tensor)*100:.1f}%)")
        print(f"   Validation: {len(X_val):,} sequences ({len(X_val)/len(X_tensor)*100:.1f}%)")
        
        # Create datasets
        train_dataset = TensorDataset(X_train, y_regr_train, y_class_train)
        val_dataset = TensorDataset(X_val, y_regr_val, y_class_val)
        
        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        print(f"üì¶ Created data loaders:")
        print(f"   Training batches: {len(train_loader)}")
        print(f"   Validation batches: {len(val_loader)}")
        print(f"   Batch size: {BATCH_SIZE}")
        
        return train_loader, val_loader
        
    except Exception as e:
        print(f"üí• Error creating data loaders: {e}")
        traceback.print_exc()
        return None, None

def train_model(model, train_loader, val_loader, model_type="enhanced", device="cpu"):
    """Train the model with validation and early stopping"""
    
    print(f"\nüéØ TRAINING {model_type.upper()} MODEL")
    print("-" * 50)
    
    try:
        # Loss functions
        if model_type == "enhanced":
            regr_criterion = UncertaintyLoss()
            class_criterion = nn.CrossEntropyLoss()
        else:
            regr_criterion = nn.MSELoss()
            class_criterion = nn.CrossEntropyLoss()
        
        # Optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
        
        # Training tracking
        best_val_loss = float('inf')
        patience = 0
        max_patience = 10
        train_losses = []
        val_losses = []
        
        print(f"üöÄ Starting training for {EPOCHS} epochs...")
        print(f"   Device: {device}")
        print(f"   Learning rate: {LEARNING_RATE}")
        print(f"   Early stopping patience: {max_patience}")
        
        model.train()
        
        for epoch in range(EPOCHS):
            # Training phase
            epoch_train_loss = 0
            epoch_regr_loss = 0
            epoch_class_loss = 0
            
            for batch_idx, (X_batch, y_regr_batch, y_class_batch) in enumerate(train_loader):
                X_batch = X_batch.to(device)
                y_regr_batch = y_regr_batch.to(device)
                y_class_batch = y_class_batch.to(device)
                
                optimizer.zero_grad()
                
                if model_type == "enhanced":
                    pred_regr, pred_class, uncertainty, confidence, _ = model(X_batch)
                    regr_loss = regr_criterion(pred_regr, y_regr_batch, uncertainty)
                    # Add confidence regularization
                    conf_reg = torch.mean((confidence - 0.5) ** 2) * 0.1
                else:
                    pred_regr, pred_class = model(X_batch)
                    regr_loss = regr_criterion(pred_regr, y_regr_batch)
                    conf_reg = 0
                
                class_loss = class_criterion(pred_class, y_class_batch)
                total_loss = regr_loss + class_loss + conf_reg
                
                total_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_train_loss += total_loss.item()
                epoch_regr_loss += regr_loss.item()
                epoch_class_loss += class_loss.item()
            
            # Validation phase
            model.eval()
            epoch_val_loss = 0
            val_regr_loss = 0
            val_class_loss = 0
            
            with torch.no_grad():
                for X_batch, y_regr_batch, y_class_batch in val_loader:
                    X_batch = X_batch.to(device)
                    y_regr_batch = y_regr_batch.to(device)
                    y_class_batch = y_class_batch.to(device)
                    
                    if model_type == "enhanced":
                        pred_regr, pred_class, uncertainty, confidence, _ = model(X_batch)
                    else:
                        pred_regr, pred_class = model(X_batch)
                    
                    v_regr_loss = nn.MSELoss()(pred_regr, y_regr_batch)
                    v_class_loss = nn.CrossEntropyLoss()(pred_class, y_class_batch)
                    v_total_loss = v_regr_loss + v_class_loss
                    
                    epoch_val_loss += v_total_loss.item()
                    val_regr_loss += v_regr_loss.item()
                    val_class_loss += v_class_loss.item()
            
            # Calculate averages
            avg_train_loss = epoch_train_loss / len(train_loader)
            avg_val_loss = epoch_val_loss / len(val_loader)
            avg_val_regr = val_regr_loss / len(val_loader)
            avg_val_class = val_class_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # Print progress
            print(f"Epoch {epoch+1:3d}/{EPOCHS} | "
                  f"Train: {avg_train_loss:.4f} | "
                  f"Val: {avg_val_loss:.4f} | "
                  f"Regr: {avg_val_regr:.4f} | "
                  f"Class: {avg_val_class:.4f}")
            
            # Early stopping check
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = model.state_dict().copy()
                patience = 0
                print(f"   ‚úÖ New best validation loss: {best_val_loss:.4f}")
            else:
                patience += 1
                if patience >= max_patience:
                    print(f"   ‚èπÔ∏è  Early stopping at epoch {epoch+1} (patience: {patience})")
                    break
            
            model.train()  # Back to training mode
        
        # Load best model
        model.load_state_dict(best_model_state)
        print(f"‚úÖ Training complete! Best validation loss: {best_val_loss:.4f}")
        
        return train_losses, val_losses, best_val_loss
        
    except Exception as e:
        print(f"üí• Training error: {e}")
        traceback.print_exc()
        return None, None, float('inf')

def save_model_and_scalers(model, feature_scaler, target_scaler, model_name="lstm_model_regression"):
    """Save trained model and scalers"""
    
    print(f"\nüíæ SAVING MODEL AND SCALERS")
    print("-" * 50)
    
    try:
        # Create models directory
        os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
        
        # Save model
        model_path = os.path.join(MODEL_SAVE_PATH, f"{model_name}.pth")
        torch.save({"model_state": model.state_dict()}, model_path)
        print(f"‚úÖ Model saved: {model_path}")
        
        # Save scalers
        feature_scaler_path = os.path.join(MODEL_SAVE_PATH, "scaler.pkl")
        target_scaler_path = os.path.join(MODEL_SAVE_PATH, "scaler_regression.pkl")
        
        joblib.dump(feature_scaler, feature_scaler_path)
        joblib.dump(target_scaler, target_scaler_path)
        
        print(f"‚úÖ Feature scaler saved: {feature_scaler_path}")
        print(f"‚úÖ Target scaler saved: {target_scaler_path}")
        
        # Show file sizes
        model_size = os.path.getsize(model_path) / (1024 * 1024)
        print(f"üìä Model file size: {model_size:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"üí• Error saving model: {e}")
        traceback.print_exc()
        return False

def create_ensemble_models(train_loader, val_loader, feature_scaler, target_scaler, device="cpu"):
    """Create ensemble models for improved predictions"""
    
    if not CREATE_ENSEMBLE or not TRAIN_ENHANCED:
        print("‚è≠Ô∏è  Skipping ensemble creation")
        return
    
    print(f"\nüé≠ CREATING ENSEMBLE MODELS")
    print("-" * 50)
    
    try:
        ensemble_configs = [
            {"dropout": 0.15, "name": "lstm_ensemble_1"},
            {"dropout": 0.25, "name": "lstm_ensemble_2"},
            {"dropout": 0.30, "name": "lstm_ensemble_3"}
        ]
        
        for i, config in enumerate(ensemble_configs):
            print(f"\nüé≠ Training ensemble model {i+1}/3: {config['name']}")
            
            # Create model with different dropout
            ensemble_model = AttentionLSTM(
                INPUT_FEATURES, HIDDEN_SIZE, NUM_LAYERS, 
                NUM_CLASSES, OUTPUT_STEPS, dropout=config['dropout']
            ).to(device)
            
            # Train with fewer epochs
            train_losses, val_losses, best_loss = train_model(
                ensemble_model, train_loader, val_loader, "enhanced", device
            )
            
            if train_losses is not None:
                # Save ensemble model
                save_model_and_scalers(ensemble_model, feature_scaler, target_scaler, config['name'])
                print(f"‚úÖ Ensemble model {i+1} saved with validation loss: {best_loss:.4f}")
            else:
                print(f"‚ùå Ensemble model {i+1} training failed")
        
        print(f"\nüéâ Ensemble creation complete!")
        
    except Exception as e:
        print(f"üí• Error creating ensemble: {e}")
        traceback.print_exc()

def main():
    """Main training function"""
    
    print("üöÄ Complete Enhanced LSTM Training Script v2.0")
    print("=" * 60)
    print(f"üéØ Training enhanced forex prediction models")
    print(f"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üî• Using device: {device}")
    
    # Step 1: Load and process data
    df, data_file = load_and_process_data()
    if df is None:
        print("‚ùå Data loading failed")
        return
    
    # Step 2: Create features
    data, feature_columns = create_features(df)
    if data is None:
        print("‚ùå Feature creation failed")
        return
    
    # Step 3: Create targets
    data = create_targets(data)
    if data is None:
        print("‚ùå Target creation failed")
        return
    
    # Step 4: Prepare sequences
    X_tensor, y_regr_tensor, y_class_tensor, feature_scaler, target_scaler = prepare_sequences(data, feature_columns)
    if X_tensor is None:
        print("‚ùå Sequence preparation failed")
        return
    
    # Step 5: Create data loaders
    train_loader, val_loader = create_data_loaders(X_tensor, y_regr_tensor, y_class_tensor)
    if train_loader is None:
        print("‚ùå Data loader creation failed")
        return
    
    # Step 6: Create and train main model
    if TRAIN_ENHANCED:
        print(f"\nüöÄ Creating Enhanced AttentionLSTM Model")
        model = AttentionLSTM(INPUT_FEATURES, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES, OUTPUT_STEPS).to(device)
        model_type = "enhanced"
    else:
        print(f"\nüîß Creating Original CombinedLSTM Model")
        model = CombinedLSTM(INPUT_FEATURES, HIDDEN_SIZE, 2, NUM_CLASSES, OUTPUT_STEPS).to(device)
        model_type = "original"
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"üìä Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Train main model
    train_losses, val_losses, best_loss = train_model(model, train_loader, val_loader, model_type, device)
    
    if train_losses is None:
        print("‚ùå Main model training failed")
        return
    
    # Step 7: Save main model
    if save_model_and_scalers(model, feature_scaler, target_scaler):
        print("‚úÖ Main model saved successfully")
    else:
        print("‚ùå Failed to save main model")
        return
    
    # Step 8: Create ensemble models
    if CREATE_ENSEMBLE and TRAIN_ENHANCED:
        create_ensemble_models(train_loader, val_loader, feature_scaler, target_scaler, device)
    
    # Step 9: Final summary
    print(f"\n" + "="*60)
    print("üéâ TRAINING COMPLETE!")
    print("="*60)
    
    print(f"üìä Training Summary:")
    print(f"   üìÅ Data file: {data_file}")
    print(f"   üìà Total records: {len(data):,}")
    print(f"   üîó Sequences: {len(X_tensor):,}")
    print(f"   üîß Model: {model_type}")
    print(f"   üìö Features: {INPUT_FEATURES}")
    print(f"   üéØ Output steps: {OUTPUT_STEPS}")
    print(f"   üíæ Best validation loss: {best_loss:.4f}")
    print(f"   üìÅ Models saved to: {MODEL_SAVE_PATH}")
    
    if CREATE_ENSEMBLE and TRAIN_ENHANCED:
        print(f"   üé≠ Ensemble models: 4 total (1 main + 3 ensemble)")
    
    print(f"\nüöÄ Next Steps:")
    print(f"   1. Test with daemon: python daemon.py")
    print(f"   2. Load EA in MetaTrader")
    print(f"   3. Monitor prediction accuracy")
    print(f"   4. Expected improvement: 25-40% better accuracy!")
    
    print(f"\n‚è∞ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n\nüõë Training interrupted by user")
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        traceback.print_exc()